---
# Automated setup of a Beowulf AI Cluster.

- name: Compile llama.cpp.
  hosts: cluster
  become: false
  tags: ['setup']

  vars_files: ['config.yml']

  tasks:
    - ansible.builtin.include_tasks: dependencies/rhel-based.yml
      when: ansible_os_family == 'RedHat'

    - ansible.builtin.include_tasks: dependencies/debian-based.yml
      when: ansible_os_family == 'Debian'

    - name: Ensure working directory exists.
      ansible.builtin.file:
        path: "{{ working_dir }}"
        state: directory
      become: true

    - name: Clone llama.cpp.
      ansible.builtin.git:
        repo: "{{ llama_source }}"
        dest: "{{ working_dir }}/llama.cpp"
        version: "{{ llama_version }}"
      become: true

    - name: Create llama.cpp build directory.
      ansible.builtin.file:
        path: "{{ working_dir }}/llama.cpp/build"
        state: directory
        mode: 0755
      become: true

    - name: Configure the build.
      ansible.builtin.command:
        cmd: "cmake -B build {{ llama_build_opts }}"
        chdir: "{{ working_dir }}/llama.cpp"
        creates: "{{ working_dir }}/llama.cpp/LLAMA_BUILD_COMPLETE"
      become: true

    - name: Build llama.cpp.
      ansible.builtin.command:
        cmd: cmake --build build --config Release
        chdir: "{{ working_dir }}/llama.cpp"
        creates: "{{ working_dir }}/llama.cpp/LLAMA_BUILD_COMPLETE"
      become: true

    - name: Create LLAMA_BUILD_COMPLETE file.
      ansible.builtin.file:
        path: "{{ working_dir }}/llama.cpp/LLAMA_BUILD_COMPLETE"
        state: touch
        mode: 0644
      become: true

    - name: Download a model to the models directory for testing.
      ansible.builtin.get_url:
        url: "{{ llama_test_model_url }}"
        checksum: "{{ llama_test_model_checksum }}"
        force: false
        dest: "{{ working_dir }}/llama.cpp/models/{{ llama_test_model_filename }}"
        mode: 0644
      become: true

- name: Run quick benchmark.
  hosts: cluster
  become: false
  tags: ['never', 'quick-bench']

  vars_files: ['config.yml']

  tasks:
    - name: Quick benchmark on each node.
      ansible.builtin.command:
        cmd: >
          build/bin/llama-bench
          -m "models/{{ llama_test_model_filename }}"
          {{ llama_bench_opts }}
        chdir: "{{ working_dir }}/llama.cpp"
      register: quick_benchmark_run

    - name: Output quick benchmark results.
      ansible.builtin.debug:
        var: quick_benchmark_run.stdout

- name: Run full benchmark.
  hosts: cluster
  become: false
  tags: ['never', 'full-bench']

  vars_files: ['config.yml']
  vars:
    host_ips: []

  handlers:
    - name: restart llama-rpc
      ansible.builtin.service:
        name: llama-rpc
        state: stopped
      become: true

  tasks:
    - name: Start llama.cpp in RPC mode on all secondary nodes.
      meta: noop

    - name: Generate list of host IP addresses.
      ansible.builtin.set_fact:
        host_ips: "{{ host_ips + [ hostvars[item].ansible_default_ipv4.address ] }}"
      loop: "{{ groups['cluster'] }}"

    - name: Generate RPC host list from host IP addresses.
      ansible.builtin.set_fact:
        rpc_hosts: "{{ host_ips | join(':' + llama_rpc_port + ',') }}:{{ llama_rpc_port }}"
      run_once: true

    - name: Copy the llama-rpc systemd unit file.
      template:
        src: llama-rpc.service.j2
        dest: /etc/systemd/system/llama-rpc.service
        mode: 0644
      register: llama_rpc_exporter_service
      notify: restart llama-rpc
      become: true

    - name: Reload systemd daemon if unit file is changed.
      systemd:
        daemon_reload: true
      notify: restart llama-rpc
      when: llama_rpc_exporter_service is changed
      become: true

    - name: Ensure llama-rpc is running.
      service:
        name: llama-rpc
        state: started
      become: true

    - name: Run benchmark across all nodes from first node
      ansible.builtin.command:
        cmd: >
          build/bin/llama-bench
          -m "models/{{ llama_test_model_filename }}"
          {{ llama_bench_opts }}
          --rpc {{ rpc_hosts }}
        chdir: "{{ working_dir }}/llama.cpp"
      register: full_benchmark_run
      run_once: true

    - name: Output the results.
      debug: var=full_benchmark_run.stdout
      run_once: true
