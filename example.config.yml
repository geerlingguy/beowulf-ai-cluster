---
# Working directory where llama.cpp will be compiled.
working_dir: /opt

llama_user: pi

llama_source: https://github.com/ggerganov/llama.cpp
llama_version: master
llama_build_opts: "-DGGML_VULKAN=1 -DGGML_RPC=ON"

llama_test_model_url: https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf
llama_test_model_checksum: "sha256:6c1a2b41161032677be168d354123594c0e6e67d2b9227c84f296ad037c728ff"
llama_test_model_filename: "Llama-3.2-3B-Instruct-Q4_K_M.gguf"

# See https://github.com/ggml-org/llama.cpp/blob/master/tools/llama-bench/README.md#syntax
llama_bench_opts: "-n 128 -ngl 99 -fa 1 -r 2"

llama_rpc_port: "50052"
llama_rpc_environment: "GGML_VK_PREFER_HOST_MEMORY=1"
# Note: You can force using the CPU instead of an iGPU by adding `--device CPU`
llama_rpc_start_command: "{{ working_dir }}/llama.cpp/build/bin/rpc-server --host {{ ansible_default_ipv4.address }} --port {{ llama_rpc_port }} --cache"

dllama_source: https://github.com/b4rtaz/distributed-llama.git
dllama_version: main
dllama_build_environment:
  DLLAMA_VULKAN: "1"

# See https://github.com/b4rtaz/distributed-llama?tab=readme-ov-file#-setup-root-node-by-single-command
dllama_test_model: "llama3_2_1b_instruct_q40"
dllama_worker_start_command: "{{ working_dir }}/distributed-llama/dllama worker --port 9999 --nthreads 1 --gpu-index 0"
dllama_worker_port: '9999'

exo_source: https://github.com/exo-explore/exo.git
exo_version: main
