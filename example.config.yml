---
# Working directory where llama.cpp will be compiled.
working_dir: /opt

####################
# llama.cpp settings
####################

llama_extra_packages: ['ccache', 'curl']

llama_user: "{{ ansible_user }}"

llama_source: https://github.com/ggerganov/llama.cpp
llama_version: master

# On DGX Spark / DGX OS, add '-DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc'
llama_build_opts: "-DGGML_VULKAN=1 -DGGML_RPC=ON"

# Llama 3.2:3B Test Model (1.87 GB)
llama_test_model:
  url: https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf
  checksum: "sha256:6c1a2b41161032677be168d354123594c0e6e67d2b9227c84f296ad037c728ff"
  filename: "Llama-3.2-3B-Instruct-Q4_K_M.gguf"

# The checksum is optional, and can be omitted, e.g. with Llama3.3:70B (40GB).
# Note: omitting the checksum forces Ansible to re-download the file.
#
# llama_test_model:
#   url: https://huggingface.co/bartowski/Llama-3.3-70B-Instruct-GGUF/resolve/main/Llama-3.3-70B-Instruct-Q4_K_M.gguf
#   filename: "Llama-3.3-70B-Instruct-Q4_K_M.gguf"
#
# For multi-part models (e.g. 00001-of-00003.gguf), you can specify the URL and
# filename of the first model part, and manually download the model files before
# running the playbook.

# See https://github.com/ggml-org/llama.cpp/blob/master/tools/llama-bench/README.md#syntax
llama_bench_opts: "-n 128 -ngl 199 -fa 1 -r 2"

llama_rpc_port: "50052"

# RPC hosts automatically use the first ipv4 address if you leave this empty.
# On systems with multiple network interfaces, you can manually override the
# list, e.g. "ip_address2:50052,ip_address3:50052" (excluding the first node).
llama_rpc_hosts: ""

llama_rpc_environment: "GGML_VK_PREFER_HOST_MEMORY=1"
# Note: You can force using the CPU instead of an iGPU by adding `--device CPU`
llama_rpc_start_command: >-
  {{ working_dir }}/llama.cpp/build/bin/rpc-server
  --host 0.0.0.0
  --port {{ llama_rpc_port }}
  --threads {{ ansible_processor_vcpus }}
  --cache

##############
# Exo settings
##############

exo_extra_packages: []

exo_source: https://github.com/exo-explore/exo.git
exo_version: main

############################
# distributed-llama settings
############################

dllama_user: root

dllama_source: https://github.com/b4rtaz/distributed-llama.git
dllama_version: main
dllama_build_environment:
  DLLAMA_VULKAN: "1"

# See https://github.com/b4rtaz/distributed-llama?tab=readme-ov-file#-setup-root-node-by-single-command
dllama_test_model: "llama3_2_1b_instruct_q40"
dllama_worker_start_command: "{{ working_dir }}/distributed-llama/dllama worker --port 9999 --nthreads 1 --gpu-index 0"
dllama_worker_port: '9999'
