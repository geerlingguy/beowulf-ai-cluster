---
- name: Run llama.cpp benchmark across the full cluster.
  hosts: cluster
  become: false

  vars_files: ['config.yml']
  vars:
    host_ips: []

  handlers:
    - name: restart llama-rpc
      ansible.builtin.service:
        name: llama-rpc
        state: restarted
      become: true

  tasks:
    - name: Download the benchmark model.
      ansible.builtin.get_url:
        url: "{{ llama_test_model_url }}"
        checksum: "{{ llama_test_model_checksum }}"
        force: false
        dest: "{{ working_dir }}/llama.cpp/models/{{ llama_test_model_filename }}"
        mode: 0644
      become: true

    - name: Generate list of host IP addresses.
      ansible.builtin.set_fact:
        host_ips: "{{ host_ips + [ hostvars[item].ansible_default_ipv4.address ] }}"
      loop: "{{ groups['cluster'] }}"

    - name: Generate RPC host list from host IP addresses.
      ansible.builtin.set_fact:
        rpc_hosts: "{{ host_ips | join(':' + llama_rpc_port + ',') }}:{{ llama_rpc_port }}"
      run_once: true

    - name: Copy the llama-rpc systemd unit file.
      template:
        src: ../templates/llama-rpc.service.j2
        dest: /etc/systemd/system/llama-rpc.service
        mode: 0644
      register: llama_rpc_exporter_service
      notify: restart llama-rpc
      become: true

    - name: Reload systemd daemon if unit file is changed.
      systemd:
        daemon_reload: true
      notify: restart llama-rpc
      when: llama_rpc_exporter_service is changed
      become: true

    - name: Ensure llama-rpc is running.
      service:
        name: llama-rpc
        state: started
      become: true

    - name: Run benchmark across all nodes from first node.
      ansible.builtin.command:
        cmd: >
          build/bin/llama-bench
          -m "models/{{ llama_test_model_filename }}"
          {{ llama_bench_opts }}
          --rpc {{ rpc_hosts }}
        chdir: "{{ working_dir }}/llama.cpp"
      register: llama_full_benchmark_run
      run_once: true

    - name: Output the results.
      debug: var=llama_full_benchmark_run.stdout
      run_once: true

    - name: Ensure llama-rpc is stopped.
      service:
        name: llama-rpc
        state: stopped
      become: true
