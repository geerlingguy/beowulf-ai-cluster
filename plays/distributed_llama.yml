---
- name: Set up distributed-llama.
  hosts: cluster
  become: false

  vars_files: ['../config.yml']

  tasks:
    - ansible.builtin.include_tasks: ../dependencies/rhel-based.yml
      when: ansible_os_family == 'RedHat'

    - ansible.builtin.include_tasks: ../dependencies/debian-based.yml
      when: ansible_os_family == 'Debian'

    - name: Ensure working directory exists.
      ansible.builtin.file:
        path: "{{ working_dir }}"
        state: directory
      become: true

    - name: Clone distributed-llama.
      ansible.builtin.git:
        repo: "{{ distributed_llama_source }}"
        dest: "{{ working_dir }}/distributed-llama"
        version: "{{ distributed_llama_version }}"
      become: true

    - name: Build distributed-llama.
      community.general.make:
        chdir: "{{ working_dir }}/distributed-llama"
        target: "{{ item }}"
      environment: "{{ distributed_llama_build_environment }}"
      become: true
      loop:
        - dllama
        - dllama-api

    # TODO:
    #   - Get a model: https://github.com/b4rtaz/distributed-llama?tab=readme-ov-file#-setup-root-node-by-single-command
    #     - `cd /opt/distributed-llama && sudo python launch.py llama3_2_1b_instruct_q40`
    #   - Run on worker: `./dllama worker --port 9999 --nthreads 1 --gpu-index 0`
    #   - Run on root: `./dllama inference --model models/llama3_2_1b_instruct_q40/dllama_model_llama3_2_1b_instruct_q40.m --tokenizer models/llama3_2_1b_instruct_q40/dllama_tokenizer_llama3_2_1b_instruct_q40.t --buffer-float-type q80 --prompt "Hello world" --steps 256 --nthreads 1 --workers 10.0.2.209:9999 --gpu-index 0`
    #                   ./dllama chat      --model models/llama3_2_1b_instruct_q40/dllama_model_llama3_2_1b_instruct_q40.m --tokenizer models/llama3_2_1b_instruct_q40/dllama_tokenizer_llama3_2_1b_instruct_q40.t --buffer-float-type q80 --nthreads 1 --max-seq-len 4096 --gpu-index 0
